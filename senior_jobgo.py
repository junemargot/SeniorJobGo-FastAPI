import os
import json
import asyncio
import nest_asyncio
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional, Dict
from dotenv import load_dotenv

# LangChain ê´€ë ¨ ì„í¬íŠ¸
from langchain import hub
from langchain_core.documents import Document
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import TextLoader, JSONLoader
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

# LangGraph ê´€ë ¨ ì„í¬íŠ¸
from langgraph.graph import StateGraph, END

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# FastAPI ì•± ìƒì„±
app = FastAPI()

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3000",  # React ê¸°ë³¸ ê°œë°œ ì„œë²„
        "http://localhost:5173",  # Vite ê°œë°œ ì„œë²„
        "http://127.0.0.1:5173"   # Vite ê°œë°œ ì„œë²„ (IP ì£¼ì†Œ)
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
    expose_headers=["*"]
)

# ì‚¬ì „ ì •ì˜
dictionary = [
    'ì‹œë‹ˆì–´ -> ì¥ë…„',
    'ê³ ë ¹ì -> ì¥ë…„',
    'ë…¸ì¸ -> ì¥ë…„',
    'ë‚˜ì´ ë§ì€ -> ì¥ë…„',
    'ì§ì¥ì¸ -> êµ¬ì§ì'
]

# í…ìŠ¤íŠ¸ ìŠ¤í”Œë¦¬í„° ì„¤ì •
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=300,
    chunk_overlap=20,
    separators=["\n\n", "\n"]
)

# AgentState ì •ì˜
class AgentState(Dict):
    query: str
    context: List[Document]
    answer: str
    should_rewrite: bool
    rewrite_count: int
    answers: List[str]

# ìš”ì²­ ëª¨ë¸
class ChatRequest(BaseModel):
    user_message: str
    user_profile: Optional[dict] = None
    session_id: Optional[str] = None

# ì‘ë‹µ ëª¨ë¸
class JobPosting(BaseModel):
    id: int
    title: str
    company: str
    location: str
    salary: str
    workingHours: str
    description: str
    requirements: Optional[str] = None
    benefits: Optional[str] = None
    applicationMethod: Optional[str] = None
    
class ChatResponse(BaseModel):
    type: str  # 'list' ë˜ëŠ” 'detail'
    message: str
    jobPostings: List[JobPosting]
    user_profile: Optional[dict] = None

# ë²¡í„° ìŠ¤í† ì–´ ì„¤ì •
def setup_vector_store():
    try:
        persist_directory = "./jobs_collection"
        
        # ì´ë¯¸ ìƒì„±ëœ Chroma DBê°€ ìˆëŠ”ì§€ í™•ì¸
        if os.path.exists(persist_directory) and os.path.isdir(persist_directory):
            print("ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.")
            embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
            db = Chroma(
                persist_directory=persist_directory,
                embedding_function=embeddings
            )
            count = db._collection.count()
            print(f"ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì™„ë£Œ (ë¬¸ì„œ ìˆ˜: {count})")
            if count == 0:
                print("ë¬¸ì„œê°€ ì—†ìœ¼ë¯€ë¡œ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
                os.rmdir(persist_directory)
                return setup_vector_store()
            return db
            
        print("ìƒˆë¡œìš´ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
        file_path = "./documents/jobs.json"
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"{file_path} íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        
        print(f"JSON íŒŒì¼ í¬ê¸°: {os.path.getsize(file_path) / 1024:.2f} KB")
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            job_count = len(data.get('ì±„ìš©ê³µê³ ëª©ë¡', []))
            print(f"JSON íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {job_count}ê°œì˜ ì±„ìš©ê³µê³ ")
            if job_count == 0:
                raise ValueError("ì±„ìš©ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        documents = []
        for idx, job in enumerate(data['ì±„ìš©ê³µê³ ëª©ë¡'], 1):
            metadata = {
                "title": job.get("ì±„ìš©ì œëª©", ""),
                "company": job.get("íšŒì‚¬ëª…", ""),
                "location": job.get("ê·¼ë¬´ì§€ì—­", ""),
                "salary": job.get("ê¸‰ì—¬ì¡°ê±´", "")
            }
            print(f"[{idx}/{job_count}] ë¬¸ì„œ ì²˜ë¦¬ ì¤‘ - ì§€ì—­: {metadata['location']}, ì œëª©: {metadata['title']}")
            
            content = job.get("ìƒì„¸ì •ë³´", {}).get("ì§ë¬´ë‚´ìš©", "")
            if content:
                # text_splitterë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
                chunks = text_splitter.split_text(content)
                for chunk in chunks:
                    doc = Document(
                        page_content=chunk,
                        metadata=metadata
                    )
                    documents.append(doc)
        
        print(f"\në¬¸ì„œ ë¡œë“œ ì™„ë£Œ: {len(documents)}ê°œì˜ ë¬¸ì„œ")
        if len(documents) == 0:
            raise ValueError("ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        print("ì„ë² ë”© ìƒì„± ì‹œì‘...")
        embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        
        print(f"Chroma DB ìƒì„± ì¤‘... ({persist_directory})")
        db = Chroma.from_documents(
            documents=documents,
            embedding=embeddings,
            persist_directory=persist_directory
        )
        
        print("ìƒˆë¡œìš´ ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ë° ì €ì¥ ì™„ë£Œ")
        return db
        
    except Exception as e:
        print(f"ë²¡í„° ìŠ¤í† ì–´ ì„¤ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        if os.path.exists(persist_directory):
            print(f"{persist_directory} ì‚­ì œ ì¤‘...")
            import shutil
            shutil.rmtree(persist_directory, ignore_errors=True)
        raise

# Retrieve ë…¸ë“œ
def retrieve(state: AgentState):
    query = state['query']
    
    # ì‚¬ìš©ì ì§€ì—­ ì •ë³´ í™•ì¸
    locations = ["ì„œìš¸", "ë¶€ì‚°", "ëŒ€êµ¬", "ì¸ì²œ", "ê´‘ì£¼", "ëŒ€ì „", "ìš¸ì‚°", "ì„¸ì¢…"]
    user_location = None
    for loc in locations:
        if loc in query:
            user_location = loc
            break
    
    # ê²€ìƒ‰ í•„í„° ì„¤ì •
    search_filter = None
    if user_location:
        # ì§€ì—­ëª…ì´ í¬í•¨ëœ ë¬¸ì„œë§Œ ê²€ìƒ‰ (ì‹œ/êµ°/êµ¬ í¬í•¨)
        search_filter = {
            "$or": [
                {"location": {"$contains": user_location}},
                {"location": {"$contains": f"{user_location}ì‹œ"}},
                {"location": {"$contains": f"{user_location}íŠ¹ë³„ì‹œ"}}
            ]
        }
        print(f"\nì§€ì—­ '{user_location}'ì— ëŒ€í•œ í•„í„° ì ìš© ê²€ìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    
    # í•„í„°ë¥¼ ì ìš©í•˜ì—¬ í•œ ë²ˆì— ê²€ìƒ‰
    docs = vector_store.similarity_search(
        query,
        k=5,  # ìƒìœ„ 5ê°œ ê²°ê³¼
        filter=search_filter
    )
    
    print(f"ê²€ìƒ‰ ê²°ê³¼: {len(docs)}ê°œ ë¬¸ì„œ ë°œê²¬")
    if user_location:
        print(f"- ì§€ì—­ í•„í„° '{user_location}' ì ìš©ë¨")
    
    return {'context': docs}

# Verify ë…¸ë“œ
def verify(state: AgentState) -> dict:
    context = state['context']
    query = state['query']
    
    rewrite_count = state.get('rewrite_count', 0)
    
    if rewrite_count >= 3:
        return {
            "should_rewrite": False,
            "rewrite_count": rewrite_count
        }
    
    verify_prompt = PromptTemplate.from_template("""
    ë‹¤ìŒ ë¬¸ì„œë“¤ì´ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ê¸°ì— ì¶©ë¶„í•œ ì •ë³´ë¥¼ í¬í•¨í•˜ê³  ìˆëŠ”ì§€ íŒë‹¨í•´ì£¼ì„¸ìš”.
    
    ì§ˆë¬¸: {query}
    
    ë¬¸ì„œë“¤:
    {context}
    
    ë‹µë³€ í˜•ì‹:
    - ë¬¸ì„œê°€ ì¶©ë¶„í•œ ì •ë³´ë¥¼ í¬í•¨í•˜ê³  ìˆë‹¤ë©´ "YES"
    - ë¬¸ì„œê°€ ì¶©ë¶„í•œ ì •ë³´ë¥¼ í¬í•¨í•˜ê³  ìˆì§€ ì•Šë‹¤ë©´ "NO"
    
    ë‹µë³€:
    """)
    
    verify_chain = verify_prompt | llm | StrOutputParser()
    response = verify_chain.invoke({
        "query": query,
        "context": "\n\n".join([str(doc) for doc in context])
    })
    
    return {
        "should_rewrite": "NO" in response.upper(),
        "rewrite_count": rewrite_count + 1,
        "answers": state.get('answers', [])
    }

# Rewrite ë…¸ë“œ
def rewrite(state: AgentState) -> dict:
    query = state['query']
    
    rewrite_prompt = PromptTemplate.from_template(f"""
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë³´ê³ , ìš°ë¦¬ì˜ ì‚¬ì „ì„ ì°¸ê³ í•´ì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë³€ê²½í•´ì£¼ì„¸ìš”.
ì´ë•Œ ë°˜ë“œì‹œ ì‚¬ì „ì— ìˆëŠ” ê·œì¹™ì„ ì ìš©í•´ì•¼ í•©ë‹ˆë‹¤.

ì‚¬ì „: {dictionary}

ì§ˆë¬¸: {{query}}

ë³€ê²½ëœ ì§ˆë¬¸ì„ ì¶œë ¥í•´ì£¼ì„¸ìš”:
""")
    
    rewrite_chain = rewrite_prompt | llm | StrOutputParser()
    response = rewrite_chain.invoke({'query': query})
    
    return {'query': response}

# Generate ë…¸ë“œ
def generate(state: AgentState) -> dict:
    query = state['query']
    context = state['context']
    rewrite_count = state.get('rewrite_count', 0)
    answers = state.get('answers', [])
    
    # chat_persona_promptì™€ generate_promptë¥¼ ê²°í•©
    combined_prompt = PromptTemplate.from_template(f"""
{chat_persona_prompt}

ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ êµ¬ì§ìì—ê²Œ ë„ì›€ì´ ë  ë§Œí•œ ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.
ê° ì±„ìš©ê³µê³ ì˜ ì§€ì—­ì´ ì‚¬ìš©ìê°€ ì°¾ëŠ” ì§€ì—­ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ íŠ¹íˆ ì£¼ì˜í•´ì„œ í™•ì¸í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {{question}}

ì°¸ê³ í•  ë¬¸ì„œ:
{{context}}

ë‹µë³€ í˜•ì‹:
ë°œê²¬ëœ ì±„ìš©ê³µê³ ë¥¼ ë‹¤ìŒê³¼ ê°™ì€ ì¹´ë“œ í˜•íƒœë¡œ ë³´ì—¬ì£¼ë˜, ì‹œë‹ˆì–´ êµ¬ì§ìê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ ì¹œê·¼í•˜ê³  ëª…í™•í•œ ì–¸ì–´ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”:

[êµ¬ë¶„ì„ ]
ğŸ“ [ì§€ì—­êµ¬] â€¢ [íšŒì‚¬ëª…]
[ì±„ìš©ê³µê³  ì œëª©]

ğŸ’° [ê¸‰ì—¬ì¡°ê±´]
â° [ê·¼ë¬´ì‹œê°„]
ğŸ“ [ì£¼ìš”ì—…ë¬´ ë‚´ìš© - í•œ ì¤„ë¡œ ìš”ì•½]

[êµ¬ë¶„ì„ ]

ê° ê³µê³ ë§ˆë‹¤ ìœ„ì™€ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ë³´ì—¬ì£¼ë˜, ì‹œë‹ˆì–´ êµ¬ì§ìì˜ ëˆˆë†’ì´ì— ë§ì¶° ì´í•´í•˜ê¸° ì‰½ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.
ë§ˆì§€ë§‰ì—ëŠ” "ë” ìì„¸í•œ ì •ë³´ë‚˜ ì§€ì› ë°©ë²•ì´ ê¶ê¸ˆí•˜ì‹œë‹¤ë©´ ì±„ìš©ê³µê³  ë²ˆí˜¸ë¥¼ ë§ì”€í•´ì£¼ì„¸ìš”." ë¼ëŠ” ë¬¸êµ¬ë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”.
""")
    
    rag_chain = combined_prompt | llm | StrOutputParser()
    response = rag_chain.invoke({
        "question": query,
        "context": "\n\n".join([
            f"ì œëª©: {doc.metadata.get('title', '')}\n"
            f"íšŒì‚¬: {doc.metadata.get('company', '')}\n"
            f"ì§€ì—­: {doc.metadata.get('location', '')}\n"
            f"ê¸‰ì—¬: {doc.metadata.get('salary', '')}\n"
            f"ìƒì„¸ë‚´ìš©: {doc.page_content}"
            for doc in context
        ])
    })
    
    answers.append(response)
    return {'answer': response, 'answers': answers}

def router(state: AgentState) -> str:
    if state.get("rewrite_count", 0) >= 3:
        return "generate"
    return "rewrite" if state.get("should_rewrite", False) else "generate"

# ì›Œí¬í”Œë¡œìš° ì„¤ì •
def setup_workflow():
    workflow = StateGraph(AgentState)
    
    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("retrieve", retrieve)
    workflow.add_node("verify", verify)
    workflow.add_node("rewrite", rewrite)
    workflow.add_node("generate", generate)
    
    # ì—£ì§€ ì¶”ê°€
    workflow.add_edge("retrieve", "verify")
    workflow.add_conditional_edges(
        "verify",
        router,
        {
            "rewrite": "rewrite",
            "generate": "generate"
        }
    )
    workflow.add_edge("rewrite", "retrieve")
    workflow.add_edge("generate", END)
    
    # ì‹œì‘ì  ì„¤ì •
    workflow.set_entry_point("retrieve")
    
    return workflow.compile()

# ì „ì—­ ë³€ìˆ˜ ì„¤ì •
vector_store = None
graph = None
llm = None

# ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬
conversation_history = {}

@app.on_event("startup")
async def startup_event():
    global vector_store, graph, llm
    # ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”
    vector_store = setup_vector_store()
    # LLM ì„¤ì • - GPT-3.5-turboë¡œ ë³€ê²½í•˜ì—¬ ì†ë„ ê°œì„ 
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    # ì›Œí¬í”Œë¡œìš° ì„¤ì •
    graph = setup_workflow()

@app.post("/api/v1/chat/", response_model=ChatResponse)
async def chat_endpoint(request: ChatRequest):
    try:
        print(f"ë°›ì€ ë©”ì‹œì§€: {request.user_message}")
        print(f"ì‚¬ìš©ì í”„ë¡œí•„: {request.user_profile}")
        
        # ì„¸ì…˜ IDë¡œ ëŒ€í™” ê¸°ë¡ ê´€ë¦¬
        session_id = request.session_id or "default"
        if session_id not in conversation_history:
            conversation_history[session_id] = {
                "messages": [],
                "last_context": None
            }
        
        # ì´ì „ ì»¨í…ìŠ¤íŠ¸ í™•ì¸
        history = conversation_history[session_id]
        
        # ìˆ«ìë¡œ ì‹œì‘í•˜ëŠ” ì§ˆë¬¸ì´ë©´ ì´ì „ ì»¨í…ìŠ¤íŠ¸ ì¬ì‚¬ìš©
        if request.user_message[0].isdigit() and history["last_context"]:
            print("ì´ì „ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¬ì‚¬ìš©í•©ë‹ˆë‹¤.")
            context = history["last_context"]
            
            # ìƒì„¸ ì •ë³´ ì¡°íšŒìš© í”„ë¡¬í”„íŠ¸
            detail_prompt = PromptTemplate.from_template("""
ì‚¬ìš©ìê°€ {number}ë²ˆ ì±„ìš©ê³µê³ ì˜ ìƒì„¸ ì •ë³´ë¥¼ ìš”ì²­í–ˆìŠµë‹ˆë‹¤.
í•´ë‹¹ ì±„ìš©ê³µê³ ì˜ ëª¨ë“  ì •ë³´ë¥¼ ìì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.

ì±„ìš©ê³µê³  ì •ë³´:
{job_info}

ë‹µë³€ í˜•ì‹:
[êµ¬ë¶„ì„ ]
ğŸ“ [ì§€ì—­êµ¬] â€¢ [íšŒì‚¬ëª…]
[ì±„ìš©ê³µê³  ì œëª©]

ğŸ’° ê¸‰ì—¬: [ìƒì„¸ ê¸‰ì—¬ ì •ë³´]
â° ê·¼ë¬´ì‹œê°„: [ìƒì„¸ ê·¼ë¬´ì‹œê°„]
ğŸ“‹ ì£¼ìš”ì—…ë¬´: [ìƒì„¸ ì—…ë¬´ë‚´ìš©]
ğŸ¯ ìê²©ìš”ê±´: [ì§€ì›ìê²©/ìš°ëŒ€ì‚¬í•­]
ğŸ“ ì§€ì›ë°©ë²•: [ìƒì„¸ ì§€ì›ë°©ë²•]

âœ¨ ë³µë¦¬í›„ìƒ: [ë³µë¦¬í›„ìƒ ì •ë³´]
[êµ¬ë¶„ì„ ]
""")
            
            number = int(request.user_message[0]) - 1
            if 0 <= number < len(context):
                job = context[number]
                response = detail_prompt.invoke({
                    "number": number + 1,
                    "job_info": f"ì œëª©: {job.metadata.get('title', '')}\n"
                               f"íšŒì‚¬: {job.metadata.get('company', '')}\n"
                               f"ì§€ì—­: {job.metadata.get('location', '')}\n"
                               f"ê¸‰ì—¬: {job.metadata.get('salary', '')}\n"
                               f"ìƒì„¸ë‚´ìš©: {job.page_content}"
                }).format()
            else:
                response = "ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹ ë²ˆí˜¸ì˜ ì±„ìš©ê³µê³ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        else:
            # ìƒˆë¡œìš´ ê²€ìƒ‰ ìˆ˜í–‰
            initial_state = {
                'query': request.user_message,
                'answers': [],
                'rewrite_count': 0,
                'should_rewrite': False
            }
            result = graph.invoke(initial_state)
            context = result.get('context', [])
            history["last_context"] = context
            response = result.get('answer', 'ì£„ì†¡í•©ë‹ˆë‹¤. ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.')
        
        # ëŒ€í™” ê¸°ë¡ ì—…ë°ì´íŠ¸
        history["messages"].append(f"ì‚¬ìš©ì: {request.user_message}")
        history["messages"].append(f"ì‹œìŠ¤í…œ: {response}")
        
        # ì‘ë‹µ ë³€í™˜
        job_postings = []
        if context:
            for idx, doc in enumerate(context, 1):
                job = JobPosting(
                    id=idx,
                    title=doc.metadata.get("title", ""),
                    company=doc.metadata.get("company", ""),
                    location=doc.metadata.get("location", ""),
                    salary=doc.metadata.get("salary", ""),
                    workingHours="ìƒì„¸ í˜ì´ì§€ ì°¸ì¡°",
                    description=doc.page_content[:100] + "...",  # ë¯¸ë¦¬ë³´ê¸°
                    requirements="",
                    benefits="",
                    applicationMethod=""
                )
                job_postings.append(job)
        
        return ChatResponse(
            type="list" if not request.user_message[0].isdigit() else "detail",
            message=response,
            jobPostings=job_postings,
            user_profile=request.user_profile
        )
        
    except Exception as e:
        print(f"ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000) 