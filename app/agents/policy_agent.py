# langchain ê´€ë ¨ ëª¨ë“ˆ
from langchain.agents import Tool, AgentExecutor, create_react_agent
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain_community.tools.tavily_search import TavilySearchResults

# í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬
import logging
import os
import re
from dotenv import load_dotenv
from functools import partial
from typing import Dict, List
from datetime import datetime, timedelta

# ë¡œê¹… ì„¤ì • ë³´ì™„
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('policy_agent.log')
    ]
)
logger = logging.getLogger('PolicyAgent')

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
logger.info("í™˜ê²½ë³€ìˆ˜ ë¡œë“œ ì™„ë£Œ")

# OpenAI ë° Tavily ì„¤ì •
llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0
)

search = TavilySearchResults(
    api_key=os.getenv("TAVILY_API_KEY"),
    max_results=3,
    search_depth="advanced",
    include_raw_content=True,
    include_domains=[
        "mohw.go.kr",     # ë³´ê±´ë³µì§€ë¶€
        "korea.kr",       # ì •ì±…ë¸Œë¦¬í•‘
        "moel.go.kr",     # ê³ ìš©ë…¸ë™ë¶€
        "kordi.or.kr",    # í•œêµ­ë…¸ì¸ì¸ë ¥ê°œë°œì›
        "bokjiro.go.kr"  # ë³µì§€ë¡œ
        # "nps.or.kr",      # êµ­ë¯¼ì—°ê¸ˆ
        # "work.go.kr"      # ì›Œí¬ë„·
    ],
    exclude_domains=[
        "wikipedia.org", "youtube.com", "facebook.com", "twitter.com"
    ],
    time_frame="3m"
)

POLICY_EXTRACTION_PROMPT = """**ì •ì±… ì •ë³´ ì¶”ì¶œ ìš”ì²­**  
ë‹¤ìŒ ì›¹ í˜ì´ì§€ì—ì„œ **ê³ ë ¹ì¸µ ê´€ë ¨ ì •ì±… ì •ë³´**ë¥¼ ë¶„ì„í•˜ì—¬ ì •ë¦¬í•´ ì£¼ì„¸ìš”.  
í•„ìˆ˜ í•­ëª©ì„ ì •í™•í•˜ê²Œ ì¶”ì¶œí•˜ê³ , ì°¾ì„ ìˆ˜ ì—†ëŠ” ì •ë³´ëŠ” `"ì •ë³´ ì—†ìŒ"`ìœ¼ë¡œ í‘œì‹œí•˜ì„¸ìš”. 


 **ì‘ë‹µ í˜•ì‹**

[ê²°ê³¼ 1] 
- ì¶œì²˜: (ê¸°ê´€ëª…)
- ì œëª©: (ì œëª©)
- ì§€ì› ëŒ€ìƒ: (ì—°ë ¹, ìê²©ìš”ê±´ ë“±. ì—†ìœ¼ë©´ `"ê³ ë ¹ì¸µ ëŒ€ìƒ"`)
- ì£¼ìš” ë‚´ìš©: (í•µì‹¬ ë‚´ìš© ìš”ì•½)
- ì‹ ì²­ ë°©ë²•: (ì‹ ì²­ ì ˆì°¨, í•„ìš” ì„œë¥˜ ë“±)
- ì—°ë½ì²˜: (ë‹´ë‹¹ê¸°ê´€ ë° ì „í™”ë²ˆí˜¸, ë¬¸ì˜ì²˜ ë“±)
- URL: (ë§í¬)

[ê²°ê³¼ 2]
- ì¶œì²˜: (ê¸°ê´€ëª…)
- ì œëª©: (ì œëª©)
- ì§€ì› ëŒ€ìƒ: (ì—°ë ¹, ìê²©ìš”ê±´ ë“±. ì—†ìœ¼ë©´ `"ê³ ë ¹ì¸µ ëŒ€ìƒ"`)
- ì£¼ìš” ë‚´ìš©: (í•µì‹¬ ë‚´ìš© ìš”ì•½)
- ì‹ ì²­ ë°©ë²•: (ì‹ ì²­ ì ˆì°¨, í•„ìš” ì„œë¥˜ ë“±)
- ì—°ë½ì²˜: (ë‹´ë‹¹ê¸°ê´€ ë° ì „í™”ë²ˆí˜¸, ë¬¸ì˜ì²˜ ë“±)
- URL: (ë§í¬)

[ê²°ê³¼ 3]
- ì¶œì²˜: (ê¸°ê´€ëª…)
- ì œëª©: (ì œëª©)
- ì§€ì› ëŒ€ìƒ: (ì—°ë ¹, ìê²©ìš”ê±´ ë“±. ì—†ìœ¼ë©´ `"ê³ ë ¹ì¸µ ëŒ€ìƒ"`)
- ì£¼ìš” ë‚´ìš©: (í•µì‹¬ ë‚´ìš© ìš”ì•½)
- ì‹ ì²­ ë°©ë²•: (ì‹ ì²­ ì ˆì°¨, í•„ìš” ì„œë¥˜ ë“±)
- ì—°ë½ì²˜: (ë‹´ë‹¹ê¸°ê´€ ë° ì „í™”ë²ˆí˜¸, ë¬¸ì˜ì²˜ ë“±)
- URL: (ë§í¬)

**ê²€ìƒ‰ì–´:** {input}

ğŸ“„ **ì›¹ í˜ì´ì§€ ë‚´ìš©:**  
{text}

 **ì£¼ì˜ ì‚¬í•­**
- ëª¨ë“  í•­ëª©ì„ ê°€ëŠ¥í•œ í•œ êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
- ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš° `"ì •ë³´ ì—†ìŒ"`ìœ¼ë¡œ í‘œê¸°í•˜ì„¸ìš”.
"""


tools = [
    Tool(
        name="Web_Search",
        # description=f"{(datetime.now() - timedelta(days=60)).strftime('%Yë…„ %mì›”')} ì´í›„ì— ë“±ë¡ëœ ì¤‘ì¥ë…„ì¸µ ê´€ë ¨ ì •ë³´ë‚˜ ë‰´ìŠ¤ë¥¼ ì›¹ì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤.",
        description="2024ë…„ 10ì›” ì´í›„ì— ë“±ë¡ëœ ì¤‘ì¥ë…„ì¸µ ê´€ë ¨ ì •ë³´ë‚˜ ë‰´ìŠ¤ë¥¼ ì›¹ì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤.",
        func=partial(search.run)  # í•¨ìˆ˜ ë°”ì¸ë”© ë¬¸ì œ í•´ê²°
    )
]

agent = create_react_agent(
    llm,
    tools,
    PromptTemplate.from_template(
        """
        ê³ ë ¹ì ì „ë¬¸ ìƒë‹´ ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤.

        ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ë“¤:
        {tools}

        ë„êµ¬ ì´ë¦„ë“¤:
        {tool_names}

        ë‹¤ìŒ ì›ì¹™ì„ ë”°ë¼ì£¼ì„¸ìš”:
        1. ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ëª…í™•í•˜ê²Œ ì„¤ëª…
        2. í•­ìƒ ê³µì‹ URLì´ë‚˜ ì¶œì²˜ ì œê³µ
        3. ì´í•´í•˜ê¸° ì‰½ê²Œ í•œêµ­ì–´ë¡œ ì‘ë‹µ

        ê²€ìƒ‰ ì‹œ ì£¼ì˜ì‚¬í•­:
        - ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ë‹¤ë¥¸ ê²€ìƒ‰ì–´ë¡œ ì‹œë„í•˜ì„¸ìš” (ì˜ˆ: ë…¸ì¸ë³µì§€, ë…¸ì¸ì¼ìë¦¬, ê³ ë ¹ì ì·¨ì—…)
        - ê°™ì€ ê²€ìƒ‰ì„ ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”
        - ì—°ë„(ì˜ˆ: 2023)ë¥¼ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”
        - í•µì‹¬ í‚¤ì›Œë“œë§Œ ê°„ë‹¨íˆ ì…ë ¥í•˜ì„¸ìš”

        ì§ˆë¬¸: {input}

        {agent_scratchpad}
        """
    )
)

agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    verbose=True,
    handle_parsing_errors=True,
    return_intermediate_steps=True,  # ì¤‘ê°„ ë‹¨ê³„ ê²°ê³¼ ë°˜í™˜
    max_iterations=1,
    max_execution_time=100
)

def extract_keywords(query: str) -> List[str]:
    """ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    # ê¸°ë³¸ í‚¤ì›Œë“œ
    keywords = ['ë…¸ì¸']
    
    # ì¶”ê°€ í‚¤ì›Œë“œ
    important_keywords = ['ì¼ìë¦¬', 'ë³µì§€', 'ì—°ê¸ˆ', 'ì·¨ì—…', 'ì§€ì›', 'ë³´í—˜', 'ê¸‰ì—¬', 'ëŒë´„']
    for keyword in important_keywords:
        if keyword in query:
            keywords.append(keyword)
    
    return keywords

def clean_text(text: str) -> str:
    """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
    # ë¶ˆí•„ìš”í•œ ê³µë°±, íŠ¹ìˆ˜ë¬¸ì ì œê±°
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'[\n\r\t]', ' ', text)
    return text.strip()

def extract_policy_info(content: str) -> Dict:
    """LLMì„ ì‚¬ìš©í•˜ì—¬ ì •ì±… ì •ë³´ ì¶”ì¶œ"""
    try:
        # ì»¨í…ì¸  ì „ì²˜ë¦¬
        content = clean_text(content)
        if len(content) > 2000:
            content = content[:2000]

        # POLICY_EXTRACTION_PROMPT ì‚¬ìš©
        messages = [
            {
                "role": "system",
                "content": POLICY_EXTRACTION_PROMPT.format(
                    # input=f"{(datetime.now() - timedelta(days=60)).strftime('%Yë…„ %mì›”')} ì´í›„ì— ë“±ë¡ëœ ì¤‘ì¥ë…„ì¸µ ê´€ë ¨ ì •ì±… ì •ë³´ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”",
                    input="2024ë…„ 10ì›” ì´í›„ì— ë“±ë¡ëœ ì¤‘ì¥ë…„ì¸µ ê´€ë ¨ ì •ì±… ì •ë³´ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”",
                    text=content  # ì›¹í˜ì´ì§€ ë‚´ìš©
                )
            }
        ]

        response = llm.invoke(messages)
        extracted_text = response.content.strip()

        # ì •ë³´ ì¶”ì¶œ ë° êµ¬ì¡°í™”
        pattern_dict = {
            "source": r"ì¶œì²˜:\s*(.+?)(?:\n|$)",
            "title": r"ì œëª©:\s*(.+?)(?:\n|$)",
            "target": r"ì§€ì› ëŒ€ìƒ:\s*(.+?)(?:\n|$)",
            "content": r"ì£¼ìš” ë‚´ìš©:\s*(.+?)(?:\n|$)",
            "applyMethod": r"ì‹ ì²­ ë°©ë²•:\s*(.+?)(?:\n|$)",
            "contact": r"ì—°ë½ì²˜:\s*(.+?)(?:\n|$)",
            "url": r"URL:\s*(.+?)(?:\n|$)"
        }

        # ê¸°ë³¸ê°’ "-"ìœ¼ë¡œ ì´ˆê¸°í™”
        policy_info = {key: "-" for key in pattern_dict}

        # ì •ê·œì‹ ì ìš©í•˜ì—¬ ì •ë³´ ì¶”ì¶œ
        for key, pattern in pattern_dict.items():
            try:
                match = re.search(pattern, extracted_text, re.MULTILINE)
                if match:
                    policy_info[key] = match.group(1).strip() if match.group(1).strip() else "-"
            except Exception as e:
                logger.error(f"{key} ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

        # ëª¨ë“  ê°’ì´ "-"ì´ë©´ None ë°˜í™˜
        if all(value == "-" for value in policy_info.values()):
            return None

        return policy_info

    except Exception as e:
        logger.error(f"ì •ì±… ì •ë³´ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

def query_policy_agent(query: str) -> Dict:
    """ì •ì±… ê²€ìƒ‰ í•¨ìˆ˜ - ìµœì í™” ë²„ì „"""
    try:
        logger.info(f"[PolicyAgent] ì •ì±… ê²€ìƒ‰ ì‹œì‘: {query}")
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = extract_keywords(query)
        enhanced_query = " ".join(keywords)
        policies = []

        try:
            web_results = search.run(enhanced_query)
            logger.info(f"[PolicyAgent] Tavily ê²€ìƒ‰ ê²°ê³¼: {len(web_results)}ê±´")
            
            if not web_results:
                return {
                    "message": "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ê²€ìƒ‰ì–´ë¡œ ì‹œë„í•´ë³´ì„¸ìš”.",
                    "policyPostings": [],
                    "type": "policy"
                }
            
            # ê²€ìƒ‰ ê²°ê³¼ê°€ ë¬¸ìì—´ë¡œ ë°˜í™˜ë˜ëŠ” ê²½ìš° ì²˜ë¦¬
            if isinstance(web_results, str):
                web_results = [{"content": web_results, "url": ""}]
            
            for item in web_results:
                try:
                    content = item.get("content", "")
                    url = item.get("url", "")
                    
                    # ë„ë©”ì¸ ì²˜ë¦¬
                    domain = url.split("/")[2].replace("www.", "").replace("m.", "")
                    domain_mapping = {
                        "korea.kr": "ëŒ€í•œë¯¼êµ­ ì •ì±…ë¸Œë¦¬í•‘",
                        "mohw.go.kr": "ë³´ê±´ë³µì§€ë¶€",
                        "moel.go.kr": "ê³ ìš©ë…¸ë™ë¶€",
                        "nps.or.kr": "êµ­ë¯¼ì—°ê¸ˆê³µë‹¨",
                        "bokjiro.go.kr": "ë³µì§€ë¡œ",
                        "work.go.kr": "ì›Œí¬ë„·",
                        "kordi.or.kr": "í•œêµ­ë…¸ì¸ì¸ë ¥ê°œë°œì›"
                    }
                    formatted_domain = domain_mapping.get(domain, domain)
                    
                    # ë¹ ë¥¸ ì •ë³´ ì¶”ì¶œ
                    policy_info = extract_policy_info(content)
                    if policy_info:
                        policy_info["source"] = formatted_domain
                        policy_info["url"] = url
                        policies.append(policy_info)
                    
                except Exception as item_error:
                    logger.error(f"[PolicyAgent] ê°œë³„ ì •ì±… ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(item_error)}")
                    continue

            return {
                "message": "ì •ì±… ì •ë³´ ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤.",
                "policyPostings": policies[:3],  # ìƒìœ„ 3ê°œë§Œ ë°˜í™˜
                "type": "policy"
            }

        except Exception as web_error:
            logger.error(f"[PolicyAgent] ê²€ìƒ‰ ì˜¤ë¥˜: {str(web_error)}")
            return {
                "message": "ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "policyPostings": [],
                "type": "policy"
            }

    except Exception as e:
        logger.error(f"[PolicyAgent] ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return {
            "message": "ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
            "policyPostings": [],
            "type": "error"
        }