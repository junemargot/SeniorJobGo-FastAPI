from typing import Dict, List
from langchain_core.documents import Document
from langchain_core.output_parsers import StrOutputParser
from langchain.prompts import ChatPromptTemplate
from langgraph.graph import StateGraph, END
from app.core.prompts import verify_prompt, rewrite_prompt, generate_prompt, chat_prompt, chat_persona_prompt
from app.utils.constants import LOCATIONS
from .chat_agent import ChatAgent

class AgentState(Dict):
    query: str
    context: List[Document]
    answer: str
    should_rewrite: bool
    rewrite_count: int
    answers: List[str]

class JobAdvisorAgent:
    def __init__(self, llm, vector_store):
        self.llm = llm
        self.vector_store = vector_store
        self.chat_agent = ChatAgent(llm)
        self.workflow = self.setup_workflow()
        
        # êµ¬ì§ ê´€ë ¨ í‚¤ì›Œë“œ
        self.job_keywords = {
            'position': ['ì¼ìžë¦¬', 'ì§ìž¥', 'ì·¨ì—…', 'ì±„ìš©', 'êµ¬ì§', 'ì¼', 'ì§ì—…', 'ì•Œë°”', 'ì•„ë¥´ë°”ì´íŠ¸', 'ì •ê·œì§', 'ê³„ì•½ì§'],
            'salary': ['ê¸‰ì—¬', 'ì›”ê¸‰', 'ì—°ë´‰', 'ì‹œê¸‰', 'ì£¼ê¸‰', 'ìž„ê¸ˆ'],
            'location': ['ì§€ì—­', 'ê·¼ì²˜', 'ê°€ê¹Œìš´', 'ë™ë„¤', 'ì‹œ', 'êµ¬', 'ë™'],
            'time': ['ì‹œê°„', 'ê·¼ë¬´ì‹œê°„', 'ê·¼ë¬´ìš”ì¼', 'ìš”ì¼', 'ì£¼ë§', 'í‰ì¼'],
            'type': ['ê²½ë¹„', 'ìš´ì „', 'ì²­ì†Œ', 'ìš”ì–‘', 'ê°„í˜¸', 'ì£¼ë°©', 'ì¡°ë¦¬', 'íŒë§¤', 'ì˜ì—…', 'ì‚¬ë¬´', 'ê´€ë¦¬', 'ìƒì‚°', 'ì œì¡°']
        }

    def is_job_related(self, query: str) -> bool:
        """êµ¬ì§ ê´€ë ¨ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìžˆëŠ”ì§€ í™•ì¸"""
        query_lower = query.lower()
        return any(
            keyword in query_lower
            for keywords in self.job_keywords.values()
            for keyword in keywords
        )

    def retrieve(self, state: AgentState):
        query = state['query']
        
        # 1. êµ¬ì§ ê´€ë ¨ í‚¤ì›Œë“œ í™•ì¸
        if not self.is_job_related(query):
            # ì¼ìƒ ëŒ€í™” ì²˜ë¦¬ -> LLMìœ¼ë¡œ ì „ë‹¬ -> êµ¬ì§ ê´€ë ¨ ëŒ€í™” ìœ ë„
            response = self.chat_agent.chat(query)
            return {
                'answer': response,
                'is_job_query': False,
                'context': [],
                'query': query
            }
        
        # 2. êµ¬ì§ ê´€ë ¨ ê²€ìƒ‰ ìˆ˜í–‰
        docs = self.vector_store.similarity_search(query, k=10)
        print(f"ê²€ìƒ‰ ê²°ê³¼: {len(docs)}ê°œ ë¬¸ì„œ")
        
        # 3. ì§€ì—­ ê¸°ë°˜ í•„í„°ë§
        user_location = next((loc for loc in LOCATIONS if loc in query), None)
        if user_location:
            filtered_docs = [
                doc for doc in docs
                if any(user_location in loc for loc in [
                    doc.metadata.get("location", ""),
                    f"{user_location}ì‹œ",
                    f"{user_location}íŠ¹ë³„ì‹œ"
                ])
            ]
            if filtered_docs:
                docs = filtered_docs[:3]
        else:
            docs = docs[:3]
            
        # 4. ê²€ìƒ‰ ê²°ê³¼ê°€ ìžˆëŠ” ê²½ìš°
        if docs:
            context_str = "\n\n".join([
                f"ì œëª©: {doc.metadata.get('title', '')}\n"
                f"íšŒì‚¬: {doc.metadata.get('company', '')}\n"
                f"ì§€ì—­: {doc.metadata.get('location', '')}\n"
                f"ê¸‰ì—¬: {doc.metadata.get('salary', '')}\n"
                f"ìƒì„¸ë‚´ìš©: {doc.page_content}"
                for doc in docs
            ])
            
            # LLMì— ì „ë‹¬í•˜ì—¬ ì‘ë‹µ ìƒì„±
            rag_chain = generate_prompt | self.llm | StrOutputParser()
            response = rag_chain.invoke({
                "question": query,
                "context": context_str
            })
            
            return {
                'answer': response,
                'is_job_query': True,
                'context': docs,
                'query': query
            }
        
        # 5. ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš°
        return {
            'answer': "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ êµ¬ì¸ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§€ì—­ì´ë‚˜ ì§ì¢…ìœ¼ë¡œ ê²€ìƒ‰í•´ë³´ì‹œê² ì–´ìš”? ì–´ë–¤ ì¢…ë¥˜ì˜ ì¼ìžë¦¬ë¥¼ ì°¾ê³  ê³„ì‹ ì§€ ë§ì”€í•´ ì£¼ì‹œë©´ ì œê°€ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ðŸ˜Š",
            'is_job_query': True,
            'context': [],
            'query': query
        }

    def verify(self, state: AgentState) -> dict:
        if state.get('is_greeting', False):
            return {
                "should_rewrite": False,
                "rewrite_count": 0,
                "answers": state.get('answers', [])
            }
            
        context = state['context']
        query = state['query']
        rewrite_count = state.get('rewrite_count', 0)
        
        if rewrite_count >= 3:
            return {
                "should_rewrite": False,
                "rewrite_count": rewrite_count
            }
        
        verify_chain = verify_prompt | self.llm | StrOutputParser()
        response = verify_chain.invoke({
            "query": query,
            "context": "\n\n".join([str(doc) for doc in context])
        })
        
        return {
            "should_rewrite": "NO" in response.upper(),
            "rewrite_count": rewrite_count + 1,
            "answers": state.get('answers', [])
        }

    def rewrite(self, state: AgentState) -> dict:
        query = state['query']
        
        rewrite_chain = rewrite_prompt | self.llm | StrOutputParser()
        response = rewrite_chain.invoke({'query': query})
        
        return {'query': response}

    def generate(self, state: AgentState) -> dict:
        query = state['query']
        context = state.get('context', [])
        
        if state.get('is_basic_question', False):
            custom_response = state.get('custom_response')
            if custom_response:
                return {'answer': custom_response, 'answers': [custom_response]}
        
        if not context:
            return {
                'answer': "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ êµ¬ì¸ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§€ì—­ì´ë‚˜ ì§ì¢…ìœ¼ë¡œ ê²€ìƒ‰í•´ë³´ì‹œê² ì–´ìš”? ì–´ë–¤ ì¢…ë¥˜ì˜ ì¼ìžë¦¬ë¥¼ ì°¾ê³  ê³„ì‹ ì§€ ë§ì”€í•´ ì£¼ì‹œë©´ ì œê°€ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ðŸ˜Š",
                'answers': []
            }
            
        rag_chain = generate_prompt | self.llm | StrOutputParser()
        response = rag_chain.invoke({
            "question": query,
            "context": "\n\n".join([
                f"ì œëª©: {doc.metadata.get('title', '')}\n"
                f"íšŒì‚¬: {doc.metadata.get('company', '')}\n"
                f"ì§€ì—­: {doc.metadata.get('location', '')}\n"
                f"ê¸‰ì—¬: {doc.metadata.get('salary', '')}\n"
                f"ìƒì„¸ë‚´ìš©: {doc.page_content}"
                for doc in context
            ])
        })
        
        return {'answer': response, 'answers': [response]}

    def router(self, state: AgentState) -> str:
        # ê¸°ë³¸ ëŒ€í™”ë‚˜ ì¸ì‚¬ì¸ ê²½ìš° ë°”ë¡œ generateë¡œ
        if state.get('is_basic_question', False):
            return "generate"
            
        # ê²€ìƒ‰ ê²°ê³¼ê°€ ìžˆìœ¼ë©´ verifyë¡œ
        if state.get('context', []):
            return "verify"
            
        # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ generateë¡œ
        return "generate"

    def setup_workflow(self):
        workflow = StateGraph(AgentState)
        
        # ë‹¨ìˆœí™”ëœ ì›Œí¬í”Œë¡œìš°: retrieve -> END
        workflow.add_node("retrieve", self.retrieve)
        workflow.add_edge("retrieve", END)
        
        workflow.set_entry_point("retrieve")
        
        return workflow.compile() 